{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "828091a5-70cf-444a-83a5-82dd1c214867",
   "metadata": {},
   "source": [
    "## Declare the TrAct Implementation\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d8a8553-17c3-4310-849c-59443860c4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Function\n",
    "from torch.linalg import inv\n",
    "\n",
    "_DEBUG = False  # Class-level debug control. Setting True will verify correct gradient implementation\n",
    "\n",
    "def unfold3d(x, kernel_size, padding=0, stride=1, dilation=1):\n",
    "    \"\"\"\n",
    "    Perform a 3D unfold operation on a 5D tensor.\n",
    "\n",
    "    Args:\n",
    "        x: A 5D tensor of shape (batch_size, channels, depth, height, width).\n",
    "        kernel_size: A tuple of 3 integers representing the kernel size in each dimension.\n",
    "        padding: A tuple of 3 integers representing the padding in each dimension.\n",
    "        stride: A tuple of 3 integers representing the stride in each dimension.\n",
    "        dilation: A tuple of 3 integers representing the dilation in each dimension.\n",
    "    \"\"\"\n",
    "\n",
    "    # Extract dimensions\n",
    "    batch_size, channels, depth, height, width = x.size()\n",
    "\n",
    "    # Apply padding\n",
    "    if padding:\n",
    "        x = nn.functional.pad(x, (padding[2], padding[2], padding[1], padding[1], padding[0], padding[0]))\n",
    "\n",
    "    # Unfold in the depth dimension\n",
    "    x = x.unfold(2, kernel_size[0], stride[0])\n",
    "    x = x.unfold(3, kernel_size[1], stride[1])\n",
    "    x = x.unfold(4, kernel_size[2], stride[2])\n",
    "\n",
    "    # Permute dimensions to arrange the kernel elements in the channel dimension\n",
    "    # New shape: (B, C, out_d, out_h, out_w, kD, kH, kW)\n",
    "    x = x.permute(0, 1, 5, 6, 7, 2, 3, 4).contiguous()\n",
    "\n",
    "    # Reshape to combine kernel elements into the channel dimension\n",
    "    # New shape: (B, C * kD * kH * kW, out_d * out_h * out_w)\n",
    "    x = x.view(batch_size, channels * kernel_size[0] * kernel_size[1] * kernel_size[2], -1)\n",
    "\n",
    "    return x\n",
    "\n",
    "class TrActFunction(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input, weight, bias, lambda_, is_conv, conv_params):\n",
    "        \"\"\"\n",
    "        Custom forward pass for TrACT.\n",
    "\n",
    "        Args:\n",
    "            input (torch.Tensor): Input tensor.\n",
    "            weight (torch.Tensor): Weight parameter.\n",
    "            bias (torch.Tensor): Bias parameter.\n",
    "            lambda_ (float): Regularization parameter.\n",
    "            is_conv (bool): Whether the layer is a convolutional layer.\n",
    "            conv_params (dict): Convolutional parameters (stride, padding, dilation, groups).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The output of the layer.\n",
    "        \"\"\"\n",
    "        ctx.save_for_backward(input, weight, bias)\n",
    "        ctx.lambda_ = lambda_\n",
    "        ctx.is_conv = is_conv\n",
    "        ctx.conv_params = conv_params\n",
    "\n",
    "        if is_conv:\n",
    "            stride, padding, dilation, groups, dim = conv_params\n",
    "            if dim == 1:\n",
    "                output = torch.nn.functional.conv1d(input, weight, bias, stride, padding, dilation, groups)\n",
    "            elif dim == 2:\n",
    "                output = torch.nn.functional.conv2d(input, weight, bias, stride, padding, dilation, groups)\n",
    "            elif dim == 3:\n",
    "                output = torch.nn.functional.conv3d(input, weight, bias, stride, padding, dilation, groups)\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported convolution dimension: {dim}\")\n",
    "        else:\n",
    "            output = input @ weight.T\n",
    "            if bias is not None:\n",
    "                output += bias\n",
    "        return output\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        input, weight, bias = ctx.saved_tensors\n",
    "        is_conv = ctx.is_conv\n",
    "        conv_params = ctx.conv_params\n",
    "\n",
    "        if is_conv:\n",
    "            # Unpack convolutional parameters\n",
    "            stride, padding, dilation, groups, dim = conv_params\n",
    "            kernel_size = weight.shape[2:]  # Kernel shape (kW) or (kH, kW) or (kD, kH, kW)\n",
    "\n",
    "            if dim == 1:\n",
    "                input_unfolded = torch.nn.functional.unfold(input.unsqueeze(-1), kernel_size=(kernel_size[0], 1),\n",
    "                                                            dilation=(dilation[0], 1), padding=(padding[0], 0),\n",
    "                                                            stride=(stride[0], 1)).squeeze(-1)\n",
    "            elif dim == 2:\n",
    "                input_unfolded = torch.nn.functional.unfold(input, kernel_size, dilation, padding, stride)\n",
    "            elif dim == 3:\n",
    "                input_unfolded = unfold3d(input, kernel_size=kernel_size, dilation=dilation,\n",
    "                                                            padding=padding, stride=stride)\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported convolution dimension: {dim}\")\n",
    "\n",
    "            # Flatten grad_output for weight gradient computation\n",
    "            grad_output_unfolded = grad_output.permute(0, *range(2, 2 + dim), 1).reshape(-1, grad_output.shape[1])\n",
    "\n",
    "            # Prepare input_unfolded for TrACT adjustment\n",
    "            input_unfolded_flat = input_unfolded.permute(0, 2, 1).reshape(-1, input_unfolded.shape[1])\n",
    "\n",
    "            # TrAct adjustment\n",
    "            b, n = input_unfolded_flat.shape\n",
    "            reg_term = ctx.lambda_ * torch.eye(n, device=input.device)\n",
    "            xTx = input_unfolded_flat.T @ input_unfolded_flat / b\n",
    "            inv_term = torch.linalg.inv(xTx + reg_term)\n",
    "\n",
    "            # Compute TrAct-adjusted weight gradient\n",
    "            if _DEBUG:\n",
    "                grad_weight = grad_output_unfolded.T @ input_unfolded_flat\n",
    "            else:\n",
    "                grad_weight = grad_output_unfolded.T @ input_unfolded_flat @ inv_term\n",
    "            grad_weight = grad_weight.view(weight.shape)  # Reshape back to original weight shape\n",
    "            \n",
    "            # Compute bias gradient\n",
    "            grad_bias = grad_output.sum(dim=(0, *range(2, 2 + dim))) if bias is not None else None\n",
    "\n",
    "        else:\n",
    "            # Handle B, *, C for Linear\n",
    "            input_flat = input.view(-1, input.shape[-1])  # Flatten to (B*, C)\n",
    "            grad_output_flat = grad_output.view(-1, grad_output.shape[-1])  # Flatten to (B*, M)\n",
    "\n",
    "            b, n = input_flat.shape  # Batch size and input features\n",
    "            reg_term = ctx.lambda_ * torch.eye(n, device=input.device)\n",
    "            xTx = input_flat.T @ input_flat / b\n",
    "            inv_term = torch.linalg.inv(xTx + reg_term)\n",
    "\n",
    "            if _DEBUG:\n",
    "                grad_weight = grad_output_flat.T @ input_flat\n",
    "            else:\n",
    "                grad_weight = grad_output_flat.T @ input_flat @ inv_term\n",
    "                \n",
    "            grad_bias = grad_output_flat.sum(0) if bias is not None else None\n",
    "\n",
    "        # First layer, no need to propagate grad_input\n",
    "        grad_input = None\n",
    "        return grad_input, grad_weight, grad_bias, None, None, None\n",
    "\n",
    "\n",
    "class TrAct(nn.Module):\n",
    "\n",
    "    def __init__(self, module, lambda_=0.1):\n",
    "        \"\"\"\n",
    "        Wraps a given nn.Linear or nn.Conv* module and modifies its backward pass using TrACT.\n",
    "\n",
    "        Args:\n",
    "            module (nn.Module): The module to wrap (must be nn.Linear or nn.Conv*).\n",
    "            lambda_ (float): The regularization parameter for TrACT.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        if not isinstance(module, (nn.Linear, nn.Conv1d, nn.Conv2d, nn.Conv3d)):\n",
    "            raise TypeError(\"TrAct only supports nn.Linear or convolutional layers.\")\n",
    "\n",
    "        self.lambda_ = lambda_\n",
    "\n",
    "        # Transfer weight and bias to the TrACT wrapper directly\n",
    "        self.weight = module.weight\n",
    "        self.bias = module.bias if hasattr(module, \"bias\") else None\n",
    "\n",
    "        # Handle convolution parameters for Conv layers\n",
    "        self.is_conv = isinstance(module, (nn.Conv1d, nn.Conv2d, nn.Conv3d))\n",
    "        if self.is_conv:\n",
    "            self.stride = module.stride\n",
    "            self.padding = module.padding\n",
    "            self.dilation = module.dilation\n",
    "            self.groups = module.groups\n",
    "            self.dim = len(module.weight.shape) - 2  # Determine dimension (1D, 2D, or 3D)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.is_conv:\n",
    "            conv_params = (self.stride, self.padding, self.dilation, self.groups, self.dim)\n",
    "            output = TrActFunction.apply(x, self.weight, self.bias, self.lambda_, True, conv_params)\n",
    "        else:\n",
    "            output = TrActFunction.apply(x, self.weight, self.bias, self.lambda_, False, None)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e60e88ff-1bcc-4062-8b54-b0823069e701",
   "metadata": {},
   "source": [
    "## Test Gradient Implementation\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3477b7a-4340-4eaa-aa2b-cd56b15c87a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "# Example: Wrapping a linear layer\n",
    "linear_layer = nn.Linear(10, 5)\n",
    "linear_layer1 = copy.deepcopy(linear_layer)\n",
    "tract_layer = TrAct(linear_layer, lambda_=0.01)\n",
    "\n",
    "x = torch.randn(8, 15, 10)\n",
    "x1 = x.clone()\n",
    "\n",
    "# Use the registered backward function\n",
    "tract_layer(x).mean().backward()\n",
    "linear_layer1(x1).mean().backward()\n",
    "\n",
    "print(\"Weight gradients match:\", torch.allclose(tract_layer.weight.grad, linear_layer1.weight.grad))\n",
    "print(\"Bias gradients match:\", torch.allclose(tract_layer.bias.grad, linear_layer1.bias.grad))\n",
    "\n",
    "print(\"Gradient W (TrAct):\", tract_layer.weight.grad)\n",
    "print(\"Gradient W1 (TrAct):\", linear_layer1.weight.grad)\n",
    "\n",
    "print(\"Gradient B (TrAct):\", tract_layer.bias.grad)\n",
    "print(\"Gradient B1 (TrAct):\", linear_layer1.bias.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a094e8-1c56-4a3e-be18-4a0bdae9706e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "# Example: Wrapping a linear layer\n",
    "linear_layer = nn.Conv1d(10, 5, 3)\n",
    "linear_layer1 = copy.deepcopy(linear_layer)\n",
    "tract_layer = TrAct(linear_layer, lambda_=0.1)\n",
    "\n",
    "x = torch.randn(2, 10, 7)\n",
    "x1 = x.clone()\n",
    "\n",
    "# Use the registered backward function\n",
    "tract_layer(x).mean().backward()\n",
    "linear_layer1(x1).mean().backward()\n",
    "\n",
    "print(\"Weight gradients match:\", torch.allclose(tract_layer.weight.grad, linear_layer1.weight.grad))\n",
    "print(\"Bias gradients match:\", torch.allclose(tract_layer.bias.grad, linear_layer1.bias.grad))\n",
    "\n",
    "print(\"Gradient W (TrAct):\", tract_layer.weight.grad)\n",
    "print(\"Gradient W1 (TrAct):\", linear_layer1.weight.grad)\n",
    "\n",
    "print(\"Gradient B (TrAct):\", tract_layer.bias.grad)\n",
    "print(\"Gradient B1 (TrAct):\", linear_layer1.bias.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd8fe65-f99f-4d26-a8a5-212a89879c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "# Example: Wrapping a linear layer\n",
    "linear_layer = nn.Conv2d(10, 5, 3)\n",
    "linear_layer1 = copy.deepcopy(linear_layer)\n",
    "tract_layer = TrAct(linear_layer, lambda_=0.1)\n",
    "\n",
    "x = torch.randn(2, 10, 7, 7)\n",
    "x1 = x.clone()\n",
    "\n",
    "# Use the registered backward function\n",
    "tract_layer(x).view(2,-1).mean().backward()\n",
    "linear_layer1(x1).view(2,-1).mean().backward()\n",
    "\n",
    "print(\"Weight gradients match:\", torch.allclose(tract_layer.weight.grad, linear_layer1.weight.grad))\n",
    "print(\"Bias gradients match:\", torch.allclose(tract_layer.bias.grad, linear_layer1.bias.grad))\n",
    "\n",
    "print(\"Gradient W (TrAct):\", tract_layer.weight.grad)\n",
    "print(\"Gradient W1 (TrAct):\", linear_layer1.weight.grad)\n",
    "\n",
    "print(\"Gradient B (TrAct):\", tract_layer.bias.grad)\n",
    "print(\"Gradient B1 (TrAct):\", linear_layer1.bias.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c8153e-4e4b-4564-b1ec-d2c9d5ca0bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "# Example: Wrapping a linear layer\n",
    "linear_layer = nn.Conv3d(10, 5, 3)\n",
    "linear_layer1 = copy.deepcopy(linear_layer)\n",
    "tract_layer = TrAct(linear_layer, lambda_=0.1)\n",
    "\n",
    "x = torch.randn(2, 10, 7, 7, 7)\n",
    "x1 = x.clone()\n",
    "\n",
    "# Use the registered backward function\n",
    "tract_layer(x).mean().backward()\n",
    "linear_layer1(x1).mean().backward()\n",
    "\n",
    "print(\"Weight gradients match:\", torch.allclose(tract_layer.weight.grad, linear_layer1.weight.grad))\n",
    "print(\"Bias gradients match:\", torch.allclose(tract_layer.bias.grad, linear_layer1.bias.grad))\n",
    "\n",
    "print(\"Gradient W (TrAct):\", tract_layer.weight.grad)\n",
    "print(\"Gradient W1 (TrAct):\", linear_layer1.weight.grad)\n",
    "\n",
    "print(\"Gradient B (TrAct):\", tract_layer.bias.grad)\n",
    "print(\"Gradient B1 (TrAct):\", linear_layer1.bias.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a763564-235d-4b1b-98f5-90c770fd16a5",
   "metadata": {},
   "source": [
    "## Define Test Classification Models (ResNet and ViT)\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af1d12f6-6cec-4633-b9a7-fbdffd6dc069",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from timm.models.vision_transformer import Block as VitBlock # import the VitBlock to save space\n",
    "\n",
    "class MLPPredictor(nn.Module):\n",
    "    def __init__(self, in_channels, out_dim, hidden_dim=512):\n",
    "        super().__init__()\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(in_channels, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(hidden_dim, out_dim),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.mlp(x)\n",
    "\n",
    "# end MLPPredictor\n",
    "\n",
    "class ViT(nn.Module):\n",
    "    def __init__(self, img_size=32, patch_size=4, num_layers=8, d_model=64, num_heads=2, \n",
    "                     pred_dim=512, num_classes=10,\n",
    "                     train_pos_emb=True, use_cls=False, do_init=False):\n",
    "        \"\"\"\n",
    "        A configurable ViT model.\n",
    "        \n",
    "        Args:\n",
    "            img_size (int): Input image size to establish sequence length.\n",
    "            patch_size (int): Size of non-overlapping input patches.\n",
    "            num_layers (int): Number of stacked transformer blocks (L).\n",
    "            d_model (int): Embedding dimmension of the model (d).\n",
    "            num_heads (int): Number of heads per attention layer (h).\n",
    "            pred_dim (int): Intermediate projection size for the MLP predictor.\n",
    "            num_classes (int): Number of output classes.\n",
    "            train_pos_emb (bool): Use random trained PE (true) or fixed sinusoidal PE (false).\n",
    "            use_cls (bool): Use a class token for prediction (true) or avgPool->MLP (false).\n",
    "            do_init(bool): Use truncnorm init (true) or default torch init (false).\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = (img_size // patch_size) ** 2\n",
    "        self.d_model = d_model\n",
    "        self.use_cls = use_cls\n",
    "\n",
    "        # Patch embedding\n",
    "        self.patch_embed = nn.Conv2d(3, d_model, kernel_size=patch_size, stride=patch_size)\n",
    "\n",
    "        # Positional encoding\n",
    "        if train_pos_emb:\n",
    "            self.pos_embed = nn.Parameter(torch.randn(1, self.num_patches, d_model))\n",
    "        else:\n",
    "            num_tokens = self.num_patches + 1 if use_cls else self.num_patches\n",
    "            pef = self._create_pos_emb(num_tokens, d_model)\n",
    "            self.pos_embed = nn.Parameter(torch.zeros_like(pef), requires_grad=False)\n",
    "            self.pos_embed.data.copy_(pef)\n",
    "\n",
    "        # output norm\n",
    "        self.norm = nn.LayerNorm(d_model, eps=1e-6)\n",
    "        \n",
    "        if self.use_cls:\n",
    "            self.cls_token = nn.Parameter(torch.randn(1, 1, d_model))\n",
    "            self.head = nn.Linear(d_model, num_classes)\n",
    "        else:\n",
    "            self.cls_token = None\n",
    "            # MLP Predictor\n",
    "            self.head = MLPPredictor(d_model, num_classes, hidden_dim=pred_dim)\n",
    "\n",
    "        # Transformer blocks\n",
    "        self.blocks = nn.ModuleList([\n",
    "            #nn.TransformerEncoderLayer(d_model, num_heads, dim_feedforward=4 * d_model, activation='gelu')\n",
    "            VitBlock(d_model, num_heads, mlp_ratio=4., qkv_bias=False, drop_path=0., act_layer=nn.GELU)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        if do_init:\n",
    "            self.init_weights()\n",
    "    \n",
    "\n",
    "    def init_weights(self):\n",
    "        def _basic_init(module):\n",
    "            # apply to nn.Linear\n",
    "            if isinstance(module, nn.Linear):\n",
    "                nn.init.trunc_normal_(module.weight, mean=0.0, std=0.02)\n",
    "                if module.bias is not None:\n",
    "                    nn.init.zeros_(module.bias)\n",
    "\n",
    "        # initialize the linear matricies with trunc norm and zero bias\n",
    "        self.apply(_basic_init)\n",
    "\n",
    "        if self.cls_token is not None:\n",
    "            nn.init.trunc_normal_(self.cls_token, mean=0.0, std=0.02)\n",
    "\n",
    "    def _create_pos_emb(self, max_len, d_model):\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        return pe\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # Patch embedding\n",
    "        x = self.patch_embed(x)  # (B, d_model, h_patches, w_patches)\n",
    "        x = x.flatten(2).transpose(1, 2)  # (B, num_patches, d_model)\n",
    "\n",
    "        # get the shapes\n",
    "        B,S,C = x.shape\n",
    "        \n",
    "        if self.use_cls:\n",
    "            cls_token = self.cls_token.expand(B,-1,-1)\n",
    "            # add the cls token to the start\n",
    "            x = torch.cat([cls_token, x], dim=1)\n",
    "\n",
    "        # Add positional encoding\n",
    "        x = x + self.pos_embed\n",
    "\n",
    "        # Transformer layers\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "\n",
    "        # Global average pooling and classification\n",
    "        if self.use_cls:\n",
    "            x = x[:,0]\n",
    "        else:\n",
    "            x = x.mean(dim=1)  # Average pool over the patch dimension\n",
    "\n",
    "        # normalize\n",
    "        x = self.norm(x)\n",
    "        \n",
    "        # predict\n",
    "        return self.head(x)\n",
    "\n",
    "# end ViT\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.gelu(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out += self.shortcut(x)\n",
    "        return self.gelu(out)\n",
    "\n",
    "# end BasicBlock\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, channels, pred_dim=512, num_classes=10):\n",
    "        \"\"\"\n",
    "        A configurable ResNet model.\n",
    "        \n",
    "        Args:\n",
    "            block (nn.Module): Residual block class (e.g., `BasicBlock`).\n",
    "            num_blocks (tuple): Number of blocks in each stage.\n",
    "            channels (list): Output channels for each stage.\n",
    "            pred_dim (int): Intermediate projection size for the MLP predictor.\n",
    "            num_classes (int): Number of output classes.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        assert len(num_blocks) == len(channels), \"num_blocks and channels must have the same length\"\n",
    "\n",
    "        self.in_channels = channels[0]\n",
    "        \n",
    "        # Patch embedding\n",
    "        self.patch_embed = nn.Conv2d(3, self.in_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(self.in_channels)\n",
    "        self.gelu = nn.GELU()\n",
    "\n",
    "        # Residual layers\n",
    "        self.blocks = nn.ModuleList()\n",
    "        for num_block, out_channels in zip(num_blocks, channels):\n",
    "            self.blocks.append(self._make_layer(block, out_channels, num_block, stride=2 if self.in_channels != out_channels else 1))\n",
    "\n",
    "        # Pooling\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "\n",
    "        # output norm before the prediction\n",
    "        self.out_norm = nn.LayerNorm(channels[-1], eps=1e-6)\n",
    "\n",
    "        # MLP predictor\n",
    "        self.mlp_head = MLPPredictor(channels[-1], num_classes, hidden_dim = pred_dim)\n",
    "\n",
    "    def _make_layer(self, block, out_channels, num_blocks, stride):\n",
    "        strides = [stride] + [1] * (num_blocks - 1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_channels, out_channels, stride))\n",
    "            self.in_channels = out_channels\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.patch_embed(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.gelu(x)\n",
    "\n",
    "        # Pass through residual blocks\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "\n",
    "        # pooling\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "\n",
    "        # normalize and predict\n",
    "        x = self.out_norm(x)\n",
    "        return self.mlp_head(x)\n",
    "        \n",
    "# end ResNet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3296db0c-30c1-4f24-9682-cfc81237515d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate the models to train in parallel\n",
    "\n",
    "vit_model = ViT(img_size=32, \n",
    "                patch_size=4,       # 8x8 tokens for 32x32 image\n",
    "                num_layers=12, \n",
    "                d_model=128, \n",
    "                num_heads=4,         # d_k = 128/4 = 32\n",
    "                pred_dim=512, \n",
    "                num_classes=100,     # CIFAR-100\n",
    "                train_pos_emb=False, # initializing to fixed sinusoidal embeddings works better\n",
    "                use_cls=False,       # learning a MLP predictor after average pooling works better than a CLS token for this scale\n",
    "                do_init=False,       # default torch init works better for this scale / model\n",
    "               ).cuda()\n",
    "\n",
    "resnet_model = ResNet(BasicBlock, \n",
    "                  num_blocks=(2, 2, 2),     # 2 blocks per stage\n",
    "                  channels=[64, 128, 256],  # Channels for each stage\n",
    "                  pred_dim=512,   \n",
    "                  num_classes=100,          # CIFAR-100\n",
    "                ).cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "118629fc-7fe8-419d-b396-b17850b8be57",
   "metadata": {},
   "source": [
    "## Test Training Code\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e776e0-4113-4eab-b497-4d59b45a70aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from timm.data.transforms_factory import create_transform\n",
    "\n",
    "# build an augmented transform for training\n",
    "train_transform = create_transform(\n",
    "        input_size = 32,\n",
    "        is_training=True,\n",
    "        use_prefetcher=False,\n",
    "        no_aug=False,\n",
    "        scale=[0.08, 1.0],\n",
    "        ratio=[0.75, 1.33],\n",
    "        hflip=0.5,\n",
    "        vflip=0.0,\n",
    "        color_jitter=0.4,\n",
    "        auto_augment='rand-m9-mstd0.5-inc1',\n",
    "        interpolation='random',\n",
    "        mean=[0.5, 0.5, 0.5],\n",
    "        std=[0.5, 0.5, 0.5],\n",
    "        crop_pct=None,\n",
    "        tf_preprocessing=False,\n",
    "        re_prob=0.25,\n",
    "        re_mode='pixel',\n",
    "        re_count=1,\n",
    "        re_num_splits=0,\n",
    "        separate=False,\n",
    "    )\n",
    "\n",
    "# Transform: Convert to tensor and normalize\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "\n",
    "# use a batch size of 128 as per the paper\n",
    "batch_size = 128\n",
    "\n",
    "# setup the dataloaders\n",
    "train_dataset = datasets.CIFAR100(root='./data', train=True, download=True, transform=train_transform)\n",
    "test_dataset = datasets.CIFAR100(root='./data', train=False, download=True, transform=val_transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e85d2b8-670b-457c-87e8-150160882309",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from functools import partial\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.nn.functional import cross_entropy\n",
    "\n",
    "\n",
    "loss_per_step = []\n",
    "val_per_epoch = []\n",
    "\n",
    "def train_and_validate(models, train_loader, val_loader, \n",
    "                       optimizers, schedules, epochs=10, \n",
    "                       lambdas=None, label_smoothing=0.1):\n",
    "    \n",
    "    # make these global so that training can be interrupted without looking the results\n",
    "    global loss_per_step\n",
    "    global val_per_epoch\n",
    "    \n",
    "    # apply TrAct based on the lambdas\n",
    "    for model, lam in zip(models, lambdas):\n",
    "        if lam is not None:\n",
    "            print(f\"Applying TrAct to {model.__class__.__name__}\")\n",
    "            model.patch_embed = TrAct(model.patch_embed, lambda_=lam)\n",
    "        else:\n",
    "            print(f\"Skipping TrAct on {model.__class__.__name__}\")\n",
    "    # end init TrAct\n",
    "\n",
    "    # setup the optimizers and LR schedulers\n",
    "    optimizers = [opt(model.parameters()) for model, opt in zip(models,optimizers)]\n",
    "    schedulers = [sched(opt) for opt, sched in zip(optimizers, schedules)]\n",
    "\n",
    "    # Arrays to store results\n",
    "    loss_per_step = [[] for _ in models]\n",
    "    val_per_epoch = [[] for _ in models]\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        # Set models to training mode\n",
    "        for model in models:\n",
    "            model.train()\n",
    "\n",
    "        # Initialize rolling average loss\n",
    "        rolling_loss = [0.0 for _ in models]\n",
    "        step_count = 0\n",
    "\n",
    "        with tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{epochs}\") as pbar:\n",
    "            for inputs, labels in pbar:\n",
    "                inputs, labels = inputs.cuda(), labels.cuda()\n",
    "\n",
    "                # Zero gradients\n",
    "                for optimizer in optimizers:\n",
    "                    optimizer.zero_grad()\n",
    "\n",
    "                # Forward pass, compute loss, backward pass, and optimizer step\n",
    "                for i, (model, optimizer) in enumerate(zip(models, optimizers)):\n",
    "                    outputs = model(inputs)\n",
    "                    loss = cross_entropy(outputs, labels, label_smoothing=label_smoothing)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                    # Update rolling loss\n",
    "                    rolling_loss[i] = 0.99 * rolling_loss[i] + 0.01 * loss.item()\n",
    "\n",
    "                    # Save loss every 50 steps\n",
    "                    if step_count % 50 == 0:\n",
    "                        loss_per_step[i].append(loss.item())\n",
    "\n",
    "                # Update step count and tqdm\n",
    "                step_count += 1\n",
    "                pbar.set_postfix({f\"Model {i} Loss\": rolling_loss[i] for i in range(len(models))})\n",
    "        # end for batch in epoch\n",
    "\n",
    "        # Update learning rate\n",
    "        for scheduler in schedulers:\n",
    "            scheduler.step(epoch+1)\n",
    "\n",
    "        # Validation loop\n",
    "        for i, model in enumerate(models):\n",
    "            model.eval()\n",
    "            correct = 0\n",
    "            total = 0\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for inputs, labels in val_loader:\n",
    "                    inputs, labels = inputs.cuda(), labels.cuda()\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = outputs.max(1)\n",
    "                    correct += (preds == labels).sum().item()\n",
    "                    total += labels.size(0)\n",
    "\n",
    "            # Calculate and save accuracy\n",
    "            accuracy = correct / total\n",
    "            val_per_epoch[i].append(accuracy)\n",
    "            print(f\"Model {i} Validation Accuracy: {accuracy:.4f}\")\n",
    "        # end validation for model in models\n",
    "    # end for each epoch\n",
    "# end training and validation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a89419e7-0ca5-46f1-83f4-1734fdc5019c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from timm.scheduler.cosine_lr import CosineLRScheduler\n",
    "import copy\n",
    "\n",
    "def make_schedule(num_epochs=100, warmup=5, lr_base=1e-3):\n",
    "    # lr_min should be 1e-5 for a lr of 1e-3, so *1e-2\n",
    "    # warmup init lr should be 1e-6 for a lr of 1e-3, so *1e-3\n",
    "    return partial(CosineLRScheduler, \n",
    "                   t_initial=num_epochs, # number of total epochs\n",
    "                   lr_min=lr_base*1e-2, \n",
    "                   warmup_t=warmup, # number of warmup epochs\n",
    "                   warmup_lr_init=lr_base*1e-3,\n",
    "                   t_in_epochs=True)\n",
    "# end make_schedule\n",
    "\n",
    "# model order is \n",
    "# - ViT\n",
    "# - ResNet\n",
    "# - ViT + TrAct\n",
    "# - ResNet + TrAct\n",
    "\n",
    "NUM_EPOCHS = 100\n",
    "\n",
    "models = [\n",
    "    vit_model,\n",
    "    resnet_model,\n",
    "    copy.deepcopy(vit_model),    # make a deep copy to avoid collisions\n",
    "    copy.deepcopy(resnet_model), # make a deep copy to avoid collisions\n",
    "]\n",
    "\n",
    "optimizers = [\n",
    "     partial(optim.Adam, lr=1e-3, betas=(0.9, 0.999), eps=1e-8, weight_decay=0), # ViT trains better with Adam\n",
    "     partial(optim.SGD,  lr=0.08, momentum=0.9, weight_decay=0.0005),            # ResNet trains better with SGD\n",
    "     partial(optim.Adam, lr=1e-3, betas=(0.9, 0.999), eps=1e-8, weight_decay=0), # ViT trains better with Adam \n",
    "     partial(optim.SGD,  lr=0.08, momentum=0.9, weight_decay=0.0005),            # ResNet trains better with SGD\n",
    "]\n",
    "    \n",
    "schedules = [\n",
    "    make_schedule(lr_base = 1e-3, num_epochs=NUM_EPOCHS), # ViT\n",
    "    make_schedule(lr_base = 0.08, num_epochs=NUM_EPOCHS), # ResNet\n",
    "    make_schedule(lr_base = 1e-3, num_epochs=NUM_EPOCHS), # ViT\n",
    "    make_schedule(lr_base = 0.08, num_epochs=NUM_EPOCHS), # ResNet\n",
    "]\n",
    "\n",
    "lambdas = [\n",
    "    None,  # ViT\n",
    "    None, # ResNet\n",
    "    0.1,  # ViT + TrAct. Paper suggests 0.1 for ViT\n",
    "    0.05, # ResNet + TrAct. Paper suggests 0.05 for ResNet\n",
    "]\n",
    "\n",
    "\n",
    "# do the training\n",
    "train_and_validate(models, train_loader, test_loader, \n",
    "                      optimizers, schedules, epochs=NUM_EPOCHS, \n",
    "                      lambdas=lambdas, label_smoothing=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d54ac74e-7072-46d0-b9c5-af79bb72f61b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
